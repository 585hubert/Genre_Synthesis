{"cells":[{"cell_type":"markdown","metadata":{"id":"GC-i0Smvl3PF"},"source":["This string here is for testing the 3 functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXZrVvLyhQ-K"},"outputs":[],"source":["String=\"The additional effort put into the aforementioned tasks allows for assets to be created which are usable in broader contexts and scopes. A dataset which is classified according to style can be used for architectures which are specifically tailored for one specific style (in the case where Text Classification is indeed deemed to be redundant). Such classes of data would not only contain known information about prosody and text, but may also be consistent in additional features. For example, a documentary class may contain background music and multiple speakers, and thus a dataset which is categorised to be in a documentary class can also be used to train other architectures such as background noise cancellation and speaker diarisation. A dataset classified in a storytelling class can be used to train emotional speech synthesis. With adequate documentation and methodology, the necessary preparations of data and architecture can extend their application beyond the specific use case. Investigations of evaluations metrics are also at the forefront of speech synthesis (\\cite{wagner19_ssw}), thus the framework also incentivises a core research area of speech synthesis. The aim of is to have a framework that is necessarily speaker independent. One speaker, regardless of how perfect they are deemed to be for a given style of speech, is necessarily limited by anatomy, training (if applicable), vocabulary, language, and so on. If the architecture is trained on the basis of functions instead of a speaker, it can be more readily fine tuned to use any speaker. Particularly, if the context of application is within a particular company or website which seeks to have a unique and distinguishable voice, it makes more sense for the starting point to be a generic, customisable architecture that understands the text, rather than an architecture which is trained using the style of one, unique speaker.\""]},{"cell_type":"markdown","metadata":{"id":"PbyR-By56aDD"},"source":["Since this notebook came from Google Colab, here we have the function to mount the"]},{"cell_type":"markdown","metadata":{"id":"pY5TCMxg8Av0"},"source":["A rudimentary function to join all elements of a list into a single string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-4sLgTguwxN"},"outputs":[],"source":["LIST=[\"For demonstrative purposes\",\"this is a sentence broken into bits.\",\"It is almost banale to have such a function.\",\"Yet here it is anyway\"]\n","\n","def ListStr(LIST):\n","  a=' '.join(LIST)\n","  return a\n","print(ListStr(LIST))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjyRtUxv9q9H"},"outputs":[],"source":["import pandas as pd\n","filename=r\"\"\n","df=pd.read_csv(filename)\n","List=df[\"Text\"].tolist()\n","List = [x for x in List if pd.isnull(x)==False]\n","ListStr(List)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vHqmw332VQY"},"outputs":[],"source":["def Str2Sen(String):\n","  #print(len(String))\n","  ListOfSentences=[]\n","  Counter=0\n","  for i in range(len(String)):\n","    EmptyStr=''\n","    while Counter < len(String):\n","        #print(Counter, \"Counter\")\n","        #print(i,Counter)\n","        if String[Counter]=='.':\n","          EmptyStr=EmptyStr+\".\"\n","          ListOfSentences.append(EmptyStr)\n","          Counter+=1\n","          #print(\"ADDED 1\")\n","          EmptyStr=''\n","        elif String[Counter]==']':\n","          EmptyStr=EmptyStr+\"].\"\n","          ListOfSentences.append(EmptyStr)\n","          EmptyStr=''\n","          Counter+=1\n","          #print(\"ADDED 1\")\n","        else:\n","          EmptyStr=EmptyStr+String[Counter]\n","          Counter+=1\n","          #print(\"ADDED 1\")\n","        if Counter % 1000 ==0:\n","          #print((Counter*100)/len(String),\"% COMPLETED\")\n","          #print(Counter)\n","          pass\n","  return ListOfSentences\n","a=Str2Sen(String)"]},{"cell_type":"markdown","metadata":{"id":"Vj6HctxE-Yph"},"source":["Just a short confirmation that the string was broken up into multiple substrings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5p8ebC1J1wv"},"outputs":[],"source":["for i in a:\n","  print(len(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hkkb_MLTmecw"},"outputs":[],"source":["def TextBundle(List,LowerLim,UpperLim):\n","  import re\n","  Counter=0\n","  FinalList=[]\n","  Str=\"\"\n","  while Counter < len(List):\n","    #print(Counter,\"COUNTER\")\n","    if len(Str)<LowerLim or len(Str)==UpperLim:\n","      Str=Str+List[Counter]\n","      Counter+=1\n","      #print(\"ADDED 1\")\n","    elif len(Str)>=LowerLim:\n","      if len(Str)<UpperLim:\n","        FinalList.append(Str)\n","        Str=''\n","        Counter+=1\n","      elif len(Str)>UpperLim:\n","        #print(len(Str), \"LENSTR BEFORE REGEX\")\n","        Str=re.sub(r\"([^\\.]*\\.)$\",r\"\",Str)\n","        #print(len(Str), \"LENSTR AFTER REGEX\")\n","        if len(Str)>0:\n","          FinalList.append(Str)\n","          Str=\"\"\n","          Counter+=1\n","        else:\n","          Str=\"\"\n","          Counter+=1\n","  FinalList.append(Str)\n","  return FinalList\n","a=TextBundle(a,600,1000)\n","#to confirm that our code has worked\n","for i in a:\n","  print(len(i))"]},{"cell_type":"markdown","metadata":{"id":"CUT9W1EcAu-Z"},"source":["The CSV used, which are the subtitles derived from the Documentary Genre. These come from Youtube."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAQHaa5sUczW"},"outputs":[],"source":["import itertools\n","Link1=r\"/TEDTalk.csv\"\n","Link2=r\"/Documentary1.csv\"\n","Link3=r\"/Documentary2.csv\"\n","Link4=r\"/News1.csv\"\n","Link5=r\"/News2.csv\"\n","Link6=r\"/Comedy1.csv\"\n","Link7=r\"/Comedy2.csv\"\n","LinkList=[Link1,Link2,Link3,Link4,Link5,Link6,Link7]"]},{"cell_type":"markdown","metadata":{"id":"LT2jyEc_A8uq"},"source":["A function which will be used to make custom bins for the groups of sentences. The chosen range between the upper and lower limits was 400, which we divide into intervals of 50. We then group together all elements less than our lower limit to be a single bin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFIAWzU0d5LX"},"outputs":[],"source":["def RangeMake(LowerLim,UpperLim,List): #This is solely for visualisation purposes\n","  CounterLower=0\n","  Counter2,Counter3,Counter4,Counter5,Counter6,Counter7,Counter8,Counter9=0,0,0,0,0,0,0,0\n","  for i in List:\n","    if len(i)<LowerLim: #this could be done more efficiently. Alas it isn't.\n","      CounterLower+=1\n","    elif len(i)>=LowerLim and len(i) < (LowerLim+50):\n","      Counter2+=1\n","    elif len(i)>=(50+LowerLim) and len(i) < (LowerLim+100):\n","      Counter3+=1\n","    elif len(i)>=(100+LowerLim) and len(i) < (LowerLim+150):\n","      Counter4+=1\n","    elif len(i)>=(150+LowerLim) and len(i) < (LowerLim+200):\n","      Counter5+=1\n","    elif len(i)>=(200+LowerLim) and len(i) < (LowerLim+250):\n","      Counter6+=1\n","    elif len(i)>=(250+LowerLim) and len(i) < (LowerLim+300):\n","      Counter7+=1\n","    elif len(i)>=(300+LowerLim) and len(i) < (LowerLim+350):\n","      Counter8+=1\n","    elif len(i)>=(350+LowerLim) and len(i) < (LowerLim+400):\n","      Counter9+=1\n","  FinalList=[CounterLower,Counter2,Counter3,Counter4,Counter5,Counter6,Counter7,Counter8,Counter9]\n","  RelativeList=[]\n","  LenList=len(List)\n","  for i in FinalList:\n","    RelativeList.append(i/LenList)\n","  return FinalList,RelativeList,LenList"]},{"cell_type":"markdown","metadata":{"id":"WrwdvuLuB3gF"},"source":["Below is the function which takes our finished list of the groups of sentences, and turns it into a CSV file which can be used as input for machine learning.\n","\n","We first check for duplicates by converting the list into dictionary keys, since keys necessarily have to be unique.\n","\n","We then generate a parallel list which denotes the genre of text. This acts as our data labelling. We combine them both into a pandas dataframe, giving them the titles of \"Genre\" and \"Text\", and we then output it to a csv of our choice.\n","\n","Returning the CSV name is optional. This is done here to reopen our newly generated CSV at the bottom of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjVurLNT4OSo"},"outputs":[],"source":["def List2CSV(List,Genre,csvname):\n","  import pandas as pd\n","  List=list(dict.fromkeys(List)) #double checking for duplicates\n","  Genre_Tag=[Genre]*len(List)\n","  df=pd.DataFrame(list(zip(Genre_Tag, List)),columns =[\"Genre\", \"Text\"])\n","  df.to_csv(csvname)\n","  return csvname\n"]},{"cell_type":"markdown","metadata":{"id":"ejB5Qv5sC6Ff"},"source":["The function below generates a histogram based on the characters lenghts of our groups of sentences. The vast majority of this code specifies stylistic specifications."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgtNXm9_8Qyp"},"outputs":[],"source":["def LenListHistogram(List,Genre,LowerLim,UpperLim,FinalTitle):\n","  import matplotlib.pyplot as plt\n","  import matplotlib.font_manager\n","  plt.rcParams['font.family'] = 'cmr10'\n","  #plt.rcParams['font.serif'] = ['Times New Roman']\n","  fig,ax=plt.subplots(figsize=(12,7),facecolor='#0B0227')\n","  Total,Relative,Len=RangeMake(LowerLim,UpperLim,List)\n","  print(LowerLim,UpperLim)\n","  labels=[0]\n","  for i in range(LowerLim,UpperLim,50):\n","    labels.append(i)\n","  ax.bar([1,2,3,4,5,6,7,8,9],Relative,tick_label=labels,width=0.9,color='#FFF500',align='edge')\n","  ax.set_ylabel('Relative Frequencies',fontdict={'fontsize': '20', 'fontweight': '3'})\n","  ax.set_xlabel('Characters inside Sample',fontdict={'fontsize': '20', 'fontweight': '3'})\n","  ax.set_facecolor(\"#0B0227\")\n","  #ax.set_xticks([i for i in range(len(bins))])\n","  #ax.set_xticklabels(bins)\n","  TITLE=r\"Character length of sentences used for the \"+Genre+\" genre.\"\n","  ax.set_title(TITLE,color='#FFFFFF', fontdict={'fontsize': '20', 'fontweight': '3'})\n","  fig.tight_layout()\n","  for axis in ['top', 'bottom', 'left', 'right']:\n","    ax.spines[axis].set_color('#FFFFFF')\n","  for ticks in ['x','y']:\n","    ax.tick_params(axis=ticks,color='#FFFFFF',labelcolor='#FFFFFF',width=2)\n","  ax.xaxis.label.set_color('#FFFFFF')\n","  ax.yaxis.label.set_color('#FFFFFF')\n","  ax.spines['left'].set_linewidth(2)\n","  ax.spines['bottom'].set_linewidth(2)\n","  ax.spines['right'].set_visible(False)\n","  ax.spines['top'].set_visible(False)\n","  plt.xticks(fontsize=15)\n","  plt.yticks(fontsize=15)\n","  TextStr=\"Total Samples = \"+str(len(List))\n","  plt.text(8.5,0.4, TextStr, fontsize=12, bbox=dict(facecolor='#86979F'))\n","  plt.savefig(FinalTitle)\n","  return plt.show()\n","#LenListHistogram(c,\"Documentaries\",800,1200,\"DocuHistogram.png\") # use this to test the function"]},{"cell_type":"markdown","metadata":{"id":"5RleBwmHDJaX"},"source":["The master function below combines all of our previous functions. We thus input an unprocessed CSV, and output a histogram of character length, and our new CSV with processed text. The output of each is merged into a training and test CSV in a separate notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGK910Q3cTx9"},"outputs":[],"source":["#Master for separate files\n","import pandas as pd\n","def MasterFunction(csv,LowerLimit,UpperLimit,Genre,FinalHistTitle,FinalCSVTitle):\n","  print(\"ok, commenced\")\n","  df=pd.read_csv(csv)\n","  print(\"read csv\")\n","  print(len(df))\n","  #df=df.head(1500)\n","  #print(\"got first 1500 rows\")\n","  List=df[\"Text\"].tolist()\n","  print(\"to listed\")\n","  List = [x for x in List if pd.isnull(x)==False] #gets rid of empty columns\n","  print(\"removed empty columns\")\n","  A=ListStr(List)\n","  B=Str2Sen(A)\n","  C=TextBundle(B,LowerLimit,UpperLimit)\n","  print(C[0])\n","  print(len(C))\n","  D=LenListHistogram(C,Genre,LowerLimit,UpperLimit,FinalHistTitle)\n","  E=List2CSV(C,Genre,FinalCSVTitle)\n","  return D,E\n","Genres=[\"200-600\"]\n","PictureNames=[\"TotalHistrogram(200-600).png\"]\n","CSVOutputNames=[\"TotalProcessed(200-600).csv\"]\n","#for a,b,c,d in zip(LinkList,Genres,PictureNames,CSVOutputNames):\n","#  Graph,NewCSV=MasterFunction(a,200,600,b,c,d)"]},{"cell_type":"code","source":["#Master for one unified file\n","Link1=r\"/TEDTalk.csv\"\n","Link2=r\"/Documentary1.csv\"\n","Link3=r\"/Documentary2.csv\"\n","Link4=r\"/News1.csv\"\n","Link5=r\"/News2.csv\"\n","Link6=r\"/Comedy1.csv\"\n","Link7=r\"/Comedy2.csv\"\n","LinkList=[Link1,Link2,Link3,Link4,Link5,Link6,Link7]\n","import pandas as pd\n","def MasterFunction(csv,LowerLimit,UpperLimit,Genre):\n","  print(\"ok, commenced\")\n","  CombinedList=[] #list of sentences\n","  GenreColumn=[] #list of genre tags\n","  for i,j in zip(csv,Genre): #for each csv containing 1 genre:\n","    Genre_Tag=[]\n","    df=pd.read_csv(i)\n","    List=df[\"Text\"].tolist() #get the list of text\n","    List = [x for x in List if pd.isnull(x)==False] #gets rid of empty columns\n","    A=ListStr(List) #Convert all text into one massive string\n","    B=Str2Sen(A) #Split A by sentences\n","    C=TextBundle(B,LowerLimit,UpperLimit) #Bundle the senteces from B into bundles between the character lengths of lowerlimit and upperlimit\n","    CombinedList.append(C) #Append this bundle to the list of sentences\n","    Genre_Tag=[j]*len(C) #Make a corresponding list of tags\n","    GenreColumn.append(Genre_Tag) #Add to list of tags\n","  CombinedList=list(itertools.chain.from_iterable(CombinedList)) #unpack lists\n","  GenreColumn=list(itertools.chain.from_iterable(GenreColumn)) #unpack lists\n","  DF=pd.DataFrame(list(zip(GenreColumn, CombinedList)),columns =[\"Genre\", \"Text\"]) #make a new dataframe out of it\n","  df2=DF.dropna(subset=['Text']) #preprocessing\n","  df3=df2[df2.Text!=\"[]\"] #preprocessing\n","  df4=df3[df3.Text!=\"\"] #preprocessing\n","  Text=df4['Text'].tolist() #Fetch merged text column\n","  Genre=df4['Genre'] #Fetch merged genre column\n","  Genre= Genre.replace(\"Stand-Up\",\"Stand Up\")\n","  GenresAll=[TEDTalk,NewsReport,Documentary,Comedy] #check if all genres are there correctly, no duplicates/typos\n","  numbers=[\"0\",\"1\",\"2\",\"3\"]\n","  for i,j in zip(GenresAll,numbers):\n","    print(i,j)\n","    Genre= Genre.replace(i,j) #replace genre tags with numbers\n","  import sklearn\n","  import numpy as np\n","  from sklearn.model_selection import train_test_split\n","  X_train, X_val, y_train, y_val = train_test_split(Text, Genre, test_size=0.15, stratify=Genre) #train test splitting\n","  train=d pd.DataFrame(list(zip(y_train, X_train)),columns =['Genre', 'Text'])\n","  test= pd.DataFrame(list(zip(y_val, X_val)),columns =['Genre', 'Text'])\n","  print(list(dict.fromkeys((train[\"Genre\"].tolist())))) #double-triple check\n","  print(list(dict.fromkeys((test[\"Genre\"].tolist())))) #double-triple check\n","  return train,test\n","Genres=[\"TEDTalk\",\"Documentary\",\"Documentary\",\"News\",\"News\",\"Stand Up\",\"Stand Up\"]\n","train,test=MasterFunction(LinkList,200,600,Genres)\n","train.to_csv(r\"train(200-600).csv\",index=False,header=False,sep=\"<\")\n","test.to_csv(r\"test(200-600).csv\",index=False,header=False,sep=\"<\")"],"metadata":{"id":"lCw0oluHUN_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Genre']"],"metadata":{"id":"lLshdMq2lIMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(LinkList))"],"metadata":{"id":"3PW1q5pVbuJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv(r\"TotalProcessed(200-600).csv\")\n","print(df.tail())\n","df2=df.dropna(subset=['Text'])\n","df3=df2[df2.Text!=\"[]\"]\n","df4=df3[df3.Text!=\"\"]\n","Text=df4['Text'].tolist()\n","TextQuote=[]\n","Genre=df4['Genre']\n","Genre= Genre.replace(\"Stand-Up\",\"Stand Up\")\n","#print(df3)\n","GenresAll=list(dict.fromkeys(Genre))\n","print(GenresAll)\n","numbers=[\"0\",\"1\",\"2\",\"3\"]\n","for i,j in zip(GenresAll,numbers):\n","  Genre= Genre.replace(i,j)\n","import sklearn\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(Text, Genre, test_size=0.15, stratify=Genre)\n","train=df = pd.DataFrame(list(zip(y_train, X_train)),columns =['Genre', 'Text'])\n","test= pd.DataFrame(list(zip(y_val, X_val)),columns =['Genre', 'Text'])\n","print(list(dict.fromkeys((train[\"Genre\"].tolist()))))\n","print(list(dict.fromkeys((test[\"Genre\"].tolist()))))\n","#train.to_csv(r\"/train(600-1000).csv\",index=False,header=False,sep=\"<\")\n","#test.to_csv(r\"/test(600-1000).csv\",index=False,header=False,sep=\"<\")\n","#data = pd.read_csv(r\"/train(600-1000).csv\", names=[\"class\", \"texts\"],sep=\"<\")\n","#print(list(dict.fromkeys((data[\"class\"].tolist()))))"],"metadata":{"id":"QBrtaYaHWPo0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-IED69DkG6tX"},"source":["And for a small proof that the CSV generated correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjmstrWiG_fs"},"outputs":[],"source":["import pandas as pd\n","print(NewCSV)\n","df=pd.read_csv(NewCSV)\n","print(df.head())"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPglZDH/aVK0bxWLM193cv6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}